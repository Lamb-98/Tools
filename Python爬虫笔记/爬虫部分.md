## urllib_基本使用

```python
import urllib.request

# 定义url
url = 'http://www.baidu.com/'

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# 获取响应中的页面的源码
# read 方法  返回的是字节形式的二进制数据
# 将二进制数据转化为字符串
# 二进制->字符串：解码  decode('编码的格式')
content = response.read().decode('utf-8')

# 打印
print(content)
```



## urllib_1个类型和6个方法

```python
import urllib.request

url = 'http://www.baidu.com'

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(url)

# 一个类型和六个方法
# response是HTTPResponse的类型
print(type(response))

# 六个方法

# 按照一字节一字节去读
# content = response.read()
# print(content)

# 返回多少个字节
# content = response.read(5)
# print(content)

# 读取一行
# content = response.readline()
# print(content)

# content = response.readlines()
# print(content)

# 返回状态码 if is 200 对了
# print(response.getcode())

# 返回url地址
# print(response.geturl())

# 获取一个状态信息
# print(response.getheaders())
```



## urllib_下载

```python
# 下载网页
import urllib.request
#
# url_page = 'http://www.baidu.com'
#
# urllib.request.urlretrieve(url_page,'baidu.html')

# 下载图片
# url_img = 'https://gimg2.baidu.com/image_search/src=http%3A%2F%2Fc-ssl.duitang.com%2Fuploads%2Fitem%2F202004%2F20%2F20200420094845_wpbrh.thumb.1000_0.png&refer=http%3A%2F%2Fc-ssl.duitang.com&app=2002&size=f9999,10000&q=a80&n=0&g=0n&fmt=jpeg?sec=1644564484&t=a87165dfaf06de9da581a45647516533'
#
# urllib.request.urlretrieve(url_img,'Nezuko.jpg')

# 下载视频
url_video = 'https://vd4.bdstatic.com/mda-mmh32assaxk7ag86/hd/cae_h264/1639794915266355279/mda-mmh32assaxk7ag86.mp4?v_from_s=hkapp-haokan-hna&auth_key=1641974700-0-0-b680ed0258024d6837f938edec2db73e&bcevod_channel=searchbox_feed&pd=1&pt=3&logid=2100285329&vid=11970505336980286062&abtest=100254_1&klogid=2100285329'
urllib.request.urlretrieve(url_video,'Nezuko.mp4')
```



## urllib_请求对象的定制

```python
import urllib.request
url = 'https://www.baidu.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

# 因为urlopen方法中不能存储字典 所以headers不能直接传进去
# 请求对象的定制
# 注意 因为参数顺序的问题  不能直接写url 和 headers 中间还有一个data参数，要用关键字传参
request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

print(content)
```



## urllib_get请求的quote方法

```python
import urllib.request
import urllib.parse

# 将中文编码为Unicode
name = urllib.parse.quote('祢豆子')
url = 'https://www.baidu.com/s?wd=' + name

# 定制headers
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)

# 模拟浏览器向服务器发送请求
response = urllib.request.urlopen(request)

# 获取响应内容
content = response.read().decode('utf-8')

print(content)
```





## urllib_get请求的urlencode方法

```python
# urlencode应用场景：多个参数的时候

import urllib.request
import urllib.parse

url = 'https://www.baidu.com/s?'
data = {
    'wd': '祢豆子',
    'sex': '女',
    'location': '日本'
}
new_data = urllib.parse.urlencode(data)
url = url + new_data

# 请求对象定制
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}
request = urllib.request.Request(url=url, headers=headers)

# 伪装浏览器向服务器发送请求
response = urllib.request.urlopen(request)

# 获得响应内容
content = response.read().decode('utf-8')
print(content)
```





## urllib_post请求百度翻译

```python
import urllib.request
import urllib.parse
import json


url = 'https://fanyi.baidu.com/v2transapi?from=en&to=zh'

headers = {
    'Accept': '*/*',
    # 必须注释掉
    # 'Accept-Encoding': 'gzip, deflate, br',
    'Accept-Language': 'zh-CN,zh;q=0.9',
    'Connection': 'keep-alive',
    'Content-Length': '117',
    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8',
    # 关键是Cookie
    'Cookie': 'BIDUPSID=B28BC0504644F37405B3EAA84069B73C; PSTM=1640668375; BAIDUID=B28BC0504644F374F07B737A72199903:FG=1; BDUSS=RCVlpDc0VDaHlTdEtUZll6RFZ6eFRWdkgtQS15S1lDSjhrSUxzTzRlejFkQU5pRVFBQUFBJCQAAAAAAAAAAAEAAADVrzXTsKLA6779YwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPXn22H159thTU; BDUSS_BFESS=RCVlpDc0VDaHlTdEtUZll6RFZ6eFRWdkgtQS15S1lDSjhrSUxzTzRlejFkQU5pRVFBQUFBJCQAAAAAAAAAAAEAAADVrzXTsKLA6779YwAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAPXn22H159thTU; BDRCVFR[S4-dAuiWMmn]=FZ_Jfs2436CUAqWmykCULPYrWm1n1fz; delPer=0; PSINO=1; BDRCVFR[3iF-CRdS3ws]=mk3SLVN4HKm; BDORZ=FFFB88E999055A3F8A630C64834BD6D0; BDRCVFR[dG2JNJb_ajR]=mk3SLVN4HKm; BDRCVFR[tox4WRQ4-Km]=mk3SLVN4HKm; BDRCVFR[-pGxjrCMryR]=mk3SLVN4HKm; BDRCVFR[CLK3Lyfkr9D]=mk3SLVN4HKm; RT="z=1&dm=baidu.com&si=dllgvfhoc7j&ss=kyb88z8t&sl=4&tt=7bq&bcn=https%3A%2F%2Ffclog.baidu.com%2Flog%2Fweirwood%3Ftype%3Dperf&ld=ta4&ul=2esq&hd=2etx"; H_PS_PSSID=; BA_HECTOR=20002k2l052h00alub1gtt6f90q; REALTIME_TRANS_SWITCH=1; FANYI_WORD_SWITCH=1; HISTORY_SWITCH=1; SOUND_SPD_SWITCH=1; SOUND_PREFER_SWITCH=1; APPGUIDE_10_0_2=1',
    'Host': 'fanyi.baidu.com',
    'Origin': 'https://fanyi.baidu.com',
    'Referer': 'https://fanyi.baidu.com/?aldtype=16047',
    'Sec-Fetch-Dest': 'empty',
    'Sec-Fetch-Mode': 'cors',
    'Sec-Fetch-Site': 'same-origin',
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',
    'X-Requested-With': 'XMLHttpRequest',
}

data = {
    'from': 'en',
    'to': 'zh',
    'query': 'spider',
    'simple_means_flag': '3',
    'sign': '63766.268839',
    'token': '0c4beea2f37c628e0054f63fe87d3443',
    'domain': 'common'
}
# post请求的参数 必须要进行编码
data = urllib.parse.urlencode(data).encode('utf-8')

# 请求对象的定制
# post请求的参数 是不会拼接再url的后面的，而是需要放在请求对象定制的参数中
# post请求的参数 必须要进行编码
request = urllib.request.Request(url=url, data=data, headers=headers)


# 模拟浏览器访问
response = urllib.request.urlopen(request)

# 获取响应内容
content = response.read().decode('utf-8')
obj = json.loads(content)  # 转化成列表
print(obj)
```





## urllib_ajax请求豆瓣电影第一页

```python
import urllib.request

url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&start=0&limit=20'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)
content = response.read().decode('utf-8')

# 数据下载到本地
# open方法默认情况下使用的是gbk的编码，if we want to save characters，那么需要再open方法中指定编码格式为utf-8
fp = open('douban.json', 'w', encoding='utf-8')
fp.write(content)
```



## urllib_ajax请求豆瓣电影前10页

```python
import urllib.request
import urllib.parse


def create_request(page):
    base_url = 'https://movie.douban.com/j/chart/top_list?type=5&interval_id=100%3A90&action=&'
# 用Python的方法拼接
    data = {
        'start': (page - 1) * 20,
        'limit': 20
    }
    data = urllib.parse.urlencode(data)

    url = base_url + data

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }

    request = urllib.request.Request(url=url, headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content


def down_load(page, content):
    with open('douban_' + str(page) + '.json', 'w', encoding='utf-8') as fp:
        fp.write(content)
    # fp = open('douban_' + str(page) + '.json', 'w', encoding='utf-8')
    # fp.write(content)


# 开始
start_page = int(input('请输入起始页码：'))
end_page = int(input('请输入结束页码：'))

for page in range(start_page, end_page + 1):
    # 每一页的请求定制
    request = create_request(page)
    # 获取响应的数据
    content = get_content(request)
    # 下载
    down_load(page, content)
```



## urllib_ajax请求肯德基官网

```python
import urllib.request
import urllib.parse


def create_request(page):
    url = 'http://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname'
# post不用拼接url，多一个data参数
    data = {
        'cname': '北京',
        'pid': '',
        'pageIndex': page,
        'pageSize': '10',
    }

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }

    data = urllib.parse.urlencode(data).encode('utf-8')

    request = urllib.request.Request(url=url, data=data, headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
    return content


def down_load(page, content):
    with open('kfc_' + str(page) + '.json', 'w', encoding='utf-8') as fp:
        fp.write(content)


start_page = int(input('请输入起始页数：'))
end_page = int(input('请输入结束页数:'))

for page in range(start_page, end_page + 1):
    # 请求对象的定制
    request = create_request(page)
    # 获取响应数据
    content = get_content(request)
    # 下载
    down_load(page, content)

```





## 异常

```python
import urllib.request
import urllib.error

# 错网址
url = 'http://www.fdgee549481.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

try:
    request = urllib.request.Request(url=url, headers=headers)
    response = urllib.request.urlopen(request)
    content = response.read().decode('utf-8')
except urllib.error.HTTPError:
    print('系统正在维护...')
except urllib.error.URLError:
    print('很抱歉！系统正在维护...')
```





## urllib_微博的cookie

```python
import urllib.request

url = 'https://weibo.cn/6693875709/info'

headers = {
    # ':authority': 'weibo.cn',
    # ':method': 'GET',
    # ':path': '/6693875709/info',
    # ':scheme': 'https',
    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9',
    # 'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cache-control': 'max-age=0',
    # cookie是关键
    'cookie': '_T_WM=b8b3b85638a9f64f8035fa7c5a4cdbb0; SUB=_2A25M24P5DeRhGeBI4lEZ9yvLyzWIHXVsJy2xrDV6PUJbktAKLVXzkW1NRnz6OiF6jXHey6JFWSlEdk2jSHkNS5bN; SUBP=0033WrSXqPxfM725Ws9jqgMF55529P9D9W5um1hkFMJGK_Tqia8hYNX55NHD95QcSo.01hMfS054Ws4DqcjMi--4i-zRi-8Fi--fi-z7iKysi--ciKLsi-27q7tt; SSOLoginState=1642066857',
    # referer 判断当前路径是不是由上一个路径进来的，一般情况下是做图片防盗链
    'referer': 'https://weibo.cn/',
    'sec-fetch-dest': 'document',
    'sec-fetch-mode': 'navigate',
    'sec-fetch-site': 'same-origin',
    'sec-fetch-user': '?1',
    'upgrade-insecure-requests': '1',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

with open('weibo.html', 'w', encoding='utf-8') as fp:
    fp.write(content)

```





## urllib_handler的基本使用

```python
import urllib.request

url = 'http://www.baidu.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

# （1）获取handler对象
handler = urllib.request.HTTPHandler()

# （2）获取opener对象
opener = urllib.request.build_opener(handler)

# （3）调用open方法
response = opener.open(request)

content = response.read().decode('utf-8')

print(content)
```





## 代理服务器

```python
import urllib.request

url = 'http://www.baidu.com/s?wd=ip'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)
# 放代理IP进去
proxies = {
    'http:': '183.247.199.111:30001'
}

handler = urllib.request.ProxyHandler(proxies=proxies)

opener = urllib.request.build_opener(handler)

response = opener.open(request)

content = response.read().decode('utf-8')

with open('daili.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
```





## 代理池

```python
import urllib.request
# 代理池，用列表来装，里头是字典
proxies_pool = [
    {'http:': '45.77.126.194:443'},
    {'http:': '45.77.126.194:443'},
]

import random
# 随机从代理池中抽一个出来
proxies = random.choice(proxies_pool)

url = 'http://www.baidu.com/s?wd=ip'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}
# 请求对象的定制
request = urllib.request.Request(url=url, headers=headers)

handler = urllib.request.ProxyHandler(proxies=proxies)

opener = urllib.request.build_opener(handler)

response = opener.open(request)

content = response.read().decode('utf-8')

with open('daili.html', 'w', encoding='utf-8') as fp:
    fp.write(content)
```





## xpath

### 安装

（1）打开chrome浏览器，扩展程序，搜xpath helper

（2）终端进入D:\Python37\Scripts

输入指令：pip install lxml -i https://pypi.douban.com/simple



### 基本语法

1. 路径查询

​		//：查找所有子孙节点，不考虑层级关系

​		/：找直接子节点

2. 谓词查询

​		//div[@id]

​		//div[@id="maincontent"]

3. 属性查询

​		//@class

4. 模糊查询

​		//div[contains(@id, "he")]

​		//div[starts-with(@id, "he")]

5. 内容查询

​		//div/h1/text()

6. 逻辑运算

​		//div[@id="head" and @class="s_down"]

​		//title | //price

```python
from lxml import etree

# xpath解析
# （1）本地文件                                              etree.parse
# （2）服务器响应的数据 response.read().decode('utf-8')        etree.HTML()

# 解析本地文件
tree = etree.parse('70_解析_xpath的基本使用.html')

# tree.xpath('xpath路径')

# 查找ul下面的li
# li_list = tree.xpath('//body/ul/li')

# 查找所有有id属性的li标签
# text()获取标签中的内容
# li_list = tree.xpath('//body/ul/li[@id]/text()')

# 找到id为h1的li标签 单引号里头要有字符串要用双引号
# li_list = tree.xpath('//ul/li[@id="h1"]/text()')

# 查找到id为h1的li标签的class的属性值
# li = tree.xpath('//ul/li[@id="h1"]/@class')

# 查询id中包含h的li标签
# li_list = tree.xpath('//ul/li[contains(@id,"h")]/text()')

# 查询id的值以h开头的li标签
# li_list = tree.xpath('//ul/li[starts-with(@id,"h")]/text()')

# 查询id为h1和class为c1的li标签
# li_list = tree.xpath('//ul/li[@id="h1" and @class="c1"]/text()')

# 查询id为h1或l1的li标签
li_list = tree.xpath('//ul/li[@id="h1"]/text() | //ul/li[@id="l1"]/text()')

print(li_list)

```





## 解析_获取百度网站的百度一下

```python
import urllib.request

url = 'https://baidu.com'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

# 解析网页源码，来获取我们想要的数据
from lxml import etree

# 解析服务器响应的文件
tree = etree.HTML(content)

# 获取想要的数据 xpath的返回值是一个列表类型的数据
result = tree.xpath('//input[@id="su"]/@value')[0]

print(result)
```





## 解析_站长素材

```python
# response中第一页
# https://sc.chinaz.com/tupian/meinvtupian.html

# 第二页
# https://sc.chinaz.com/tupian/meinvtupian_2.html
import urllib.request
from lxml import etree


def create_request(page):
    if page == 1:
        url = 'https://sc.chinaz.com/tupian/meinvtupian.html'
    else:
        url = 'https://sc.chinaz.com/tupian/meinvtupian_' + str(page) + '.html'

    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
    }

    request = urllib.request.Request(url=url, headers=headers)
    return request


def get_content(request):
    response = urllib.request.urlopen(request)

    content = response.read().decode('utf-8')

    return content


def down_load(content):
    # 下载图片
    tree = etree.HTML(content)

    name_list = tree.xpath('//div[@id="container"]//a/img/@alt')

    # 一般设计图片的网站都会进行懒加载，要用没显示完时的src
    src_list = tree.xpath('//div[@id="container"]//a/img/@src2')

    for i in range(len(name_list)):
        name = name_list[i]

        src = src_list[i]

        url = 'https:' + src

        urllib.request.urlretrieve(url=url, filename='./beauty/' + name + '.jpg')


start_page = int(input('请输入起始页码：'))
end_page = int(input('请输入结束页码：'))

for page in range(start_page, end_page + 1):
    # 请求对象的定制
    request = create_request(page)
    # 获取网页的源码
    content = get_content(request)
    # 下载图片
    down_load(content)
```



## 解析_jsonpath

详情：https://blog.csdn.net/luxideyao/article/details/77802389

```python
import jsonpath
import json

obj = json.load(open('73_解析_jsonpath.json', 'r', encoding='utf-8'))

# # 书店所有书的作者
#
# author_list = jsonpath.jsonpath(obj, '$.store.book[*].author')
#
# print(author_list)

# # 所有的作者
#
# author_list = jsonpath.jsonpath(obj, '$..author')
# print(author_list)

# # store下面的所有元素
# tag_list = jsonpath.jsonpath(obj,'$.store.*')
# print(tag_list)

# # store里面所有东西的price
# price_list = jsonpath.jsonpath(obj,'$.store..price')
# print(price_list)

# # 第三本书
# book = jsonpath.jsonpath(obj,'$..book[2]')
# print(book)

# # 最后一本书
# book = jsonpath.jsonpath(obj, '$..book[(@.length-1)]')
# print(book)

# # 前两本书
# book_list = jsonpath.jsonpath(obj,'$..book[0,1]')
# book_list = jsonpath.jsonpath(obj,'$..book[:2]')
# print(book_list)

# # 条件过滤需要再()的前面添加一个?
# # 过滤出所有的包含isbn的书。
# book_list = jsonpath.jsonpath(obj,'$..book[?(@.isbn)]')
# print(book_list)

# 哪本书价格超过了10
book_list = jsonpath.jsonpath(obj,'$..book[?(@.price>10)]')
print(book_list)
```





## 解析_jsonpath_淘票票

```python
解析import urllib.request

url = 'https://www.taopiaopiao.com/cityAction.json?activityId&_ksTS=1644303385315_104&jsoncallback=jsonp105&action=cityAction&n_s=new&event_submit_doGetAllRegion=true'

headers = {
    # ':authority': 'www.taopiaopiao.com',
    # ':method': 'GET',
    # ':path': '/cityAction.json?activityId&_ksTS=1644303385315_104&jsoncallback=jsonp105&action=cityAction&n_s=new&event_submit_doGetAllRegion=true',
    # ':scheme': 'https',
    'accept': 'text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01',
    # 'accept-encoding': 'gzip, deflate, br',
    'accept-language': 'zh-CN,zh;q=0.9',
    'cookie': 't=35d38b2eed6c2f8b2d6de75379faefac; _tb_token_=e77e31f63ae1b; cookie2=1aff20a7410ff8381f389f40162f8b9a; tb_city=510100; tb_cityName="s8m2vA=="; l=eBgrqt4eLO-DSo3vBO5aourza779gIOb8sPzaNbMiInca1JF9F_1NNCngYWJWdtjgt5bYeKPkdi_7RE6JS4_WVt8iqShee8CeiJw8e1..; isg=BN3d6ZCKY36fxwdSHPhVL_4n7LnX-hFM1kdzeZ-jCDRjVvyIZkq3Ho5IgErQlikE',
    'referer': 'https://www.taopiaopiao.com/',
    'sec-fetch-dest': 'empty',
    'sec-fetch-mode': 'cors',
    'sec-fetch-site': 'same-origin',
    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36',
    'x-requested-with': 'XMLHttpRequest',
}

request = urllib.request.Request(url=url, headers=headers)

response = urllib.request.urlopen(request)

content = response.read().decode('utf-8')

# split 切割（切完是列表）,切掉同为括号及其之前和之后的内容
content = content.split('(')[1].split(')')[0]

with open('74_jsonpth_解析淘票票.json', 'w', encoding='utf-8') as fp:
    fp.write(content)

import json
import jsonpath
# 打开文件
obj = json.load(open('74_jsonpth_解析淘票票.json', 'r', encoding='utf-8'))

city_list = jsonpath.jsonpath(obj,'$..regionName')

print(city_list)

```





## 解析_bs4的基本使用

```python
from bs4 import BeautifulSoup

# 默认打开的文件的编码格式是gbk，所以在打开文件的时候需要指定编码
soup = BeautifulSoup(open('75_解析_bs4的基本使用.html', encoding='utf-8'), 'lxml')

# # 根据标签名查找节点
# # 找到的是第一个符合条件的数据
# print(soup.a)

# # 以字典的形式获取标签的属性和属性值
# print(soup.a.attrs)

# bs4的一些函数
# # (1)find
# # 返回第一个符合条件的数据
# print(soup.find('a'))

# # 根据title
# print(soup.find('a', title="a2"))
#
# # 根据class的值来找到对象，需要加一个_
# print(soup.find('a',class_='a1'))

# (2)find_all 返回的是一个列表，并且返回了所有的a标签
# print(soup.find_all('a'))

# 如果想获取的是多个标签的数据，那么需要再find_all的参数中添加的是列表数据
# print(soup.find_all(['a', 'span']))

# limit的作用是查找前几个数据
# print(soup.find_all('li',limit=2))

# (3)select(推荐)
# select方法返回的是一个列表，并且会返回多个数据
# print(soup.select('a'))

# 可以通过.代表class
# print(soup.select('.a1'))

# print(soup.select('#l1'))


# 属性选择器
# 查找到li标签中有id的标签
# print(soup.select('li[id]'))


# 查找到li标签中id为l2的标签
# print(soup.select('li[id="l2"]'))


# 层级选择器
# 后代选择器
# 找到的是div下面的li
# print(soup.select('div li'))

# 子代选择器
# 某标签的第一级子标签
# print(soup.select('div > ul > li'))

# 找到a标签和li标签的所有对象
# print(soup.select('a,li'))

# 节点信息
# 获取节点内容
# obj = soup.select('#d1')[0]
# 如果标签对象中只有内容那么string和get_text（）都可以使用
# 如果标签对象中除了内容还有标签那么string就获取不到数据而get_text（）是可以获取数据
# 我们一般情况下推荐使用get_text()
# print(obj.string)
# print(obj.get_text())

# 节点的属性
# obj = soup.select('#p1')[0]
# name是标签的名字
# print(obj.name)
# 将属性值作为一个字典返回
# print(obj.attrs)

# 获取节点的属性
obj = soup.select('#p1')[0]

print(obj.attrs.get('class'))
print(obj.get('class'))
print(obj['class'])
```





## 解析_bs4爬取星巴克数据

```python
import urllib.request

url = 'https://www.starbucks.com.cn/menu/'

response = urllib.request.urlopen(url)

content = response.read().decode('utf-8')

from bs4 import BeautifulSoup

soup = BeautifulSoup(content, 'lxml')

# //ul[@class="grid padded-3 product"]/li/a/strong/text()
name_list = soup.select('ul[class="grid padded-3 product"] li a strong')
print(name_list)

for name in name_list:
    print(name.string)
```





## selenium（有界面浏览器）

```python
from selenium import webdriver

# 创建浏览器操作对象
path = 'chromedriver.exe'

browser = webdriver.Chrome(path)

# 访问网站
url = 'https://www.baidu.com'

browser.get(url)

# # 新方法，没有警告
# from selenium import webdriver
# from selenium.webdriver.chrome.service import Service
#
# s = Service('chromedriver.exe')
# driver = webdriver.Chrome(service=s)
#
# driver.get('https://www.baidu.com/')
# driver.quit()

```





## selenium_元素定位（过时了）

```python
from selenium import webdriver

path= 'chromedriver.exe'
browser =webdriver.Chrome(path)

url = 'https://www.baidu.com'
browser.get(url)

# 元素定位

# 根据id来找到对象
# button = browser.find_element_by_id('su')
# print(button)

# 根据标签属性的属性值来获取对象的
# button = browser.find_element_by_name('wd')
# print(button)

# 根据xpath语句来获取对象
# button=browser.find_elements_by_xpath('//input[@id="su"]')
# print(button)

# 根据标签的名字来获取对象
# button = browser.find_elements_by_tag_name('input')
# print(button)

# 使用bs4的语法来获取对象
# button = browser.find_element_by_css_selector('#su')
# print(button)

button = browser.find_element_by_link_text('视频')
print(button)
```





## selenium_元素信息（过时了）

```python
from selenium import webdriver

path = 'chromedriver.exe'
browser = webdriver.Chrome(path)

url = 'https://www.baidu.com'
browser.get(url)

input = browser.find_element_by_id('su')
# 获取元素属性
print(input.get_attribute('class'))
# 获取标签名
print(input.tag_name)
# 获取元素文本
a = browser.find_element_by_link_text('新闻')
print(a.text)

```





## selenium_交互（会遇到验证）

```python
from selenium import webdriver

# 创建浏览器对象
path = 'chromedriver.exe'
browser = webdriver.Chrome(path)

url = 'https://www.baidu.com'
browser.get(url)

import time
time.sleep(1)

# 获取文本框的对象
input = browser.find_element_by_id('kw')

# 在文本框中输入周杰伦
input.send_keys('Nezuko')

time.sleep(2)

# 获取百度一下的按钮
button = browser.find_element_by_id('su')

# 点击按钮
button.click()

time.sleep(2)

# 滑到底部
js_button = 'document.documentElement.scrollTop=100000'
browser.execute_script(js_button)

time.sleep(3)

# 获取下一页的按钮
next = browser.find_element_by_xpath('//a[@class="n"]')

# 点击下一页
next.click()

time.sleep(2)

# 回到上一页
browser.back()

time.sleep(2)

# 回去
browser.forward()

time.sleep(3)

# 退出
browser.quit()
```





## selenium之handless（无界面浏览器）

```python
# 封装的handless
from selenium import webdriver
from selenium.webdriver.chrome.options import Options


def share_browser():
    chrome_options = Options()
    chrome_options.add_argument('--headless')
    chrome_options.add_argument('--disable-gpu')

    # path是你自己的Chrome浏览器的文件路径
    path = r'C:\Program Files\Google\Chrome\Application\chrome.exe'
    chrome_options.binary_location = path
    browser = webdriver.Chrome(chrome_options=chrome_options)
    return browser


browser = share_browser()

url = 'https://www.baidu.com'

browser.get(url)
browser.save_screenshot('baidu.png')
```





## request库基本使用

快速上手：https://cn.python-requests.org/zh_CN/latest/

```python
import requests

url = 'http://www.baidu.com'

response = requests.get(url=url)

# 一个类型六个属性
# Response类型
# print(type(response))

# 设置响应的编码格式
response.encoding = 'utf-8'

# 一字符串的形式来返回网页的源码
# print(response.text)

# 返回一个url地址
# print(response.url)

# 返回的是二进制的数据
# print(response.content)

# 返回响应的状态码
print(response.status_code)

# 返回的是响应头
print(response.headers)
```





## requests_get请求

```python
import requests

url = 'https://www.baidu.com/s?'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'
}

data = {
    'wd': '北京'
}

# params 参数
# kwargs 字典
response = requests.get(url=url, params=data, headers=headers)

content = response.text

print(content)

# 参数使用params传递
# 参数无需urlencode编码
# 不需要请求对象的定制
# url中的?可加可不加
```





## requests的post请求

```python
import requests

response = requests.post(url=, data=, headers=)
# data 请求参数
# kwargs 字典

# post请求 不需要编解码
# post请求的参数是data
# 不需要请求对象的定制

```





## requests_cookie登陆古诗文网

```python

```





## scrapy

```
1. 创建爬虫的项目  scrapy startproject 项目名
注意：项目的名字不允许使用数字开头，也不能包含中文

2. 创建爬虫文件
    要在spiders文件夹中去创建爬虫文件
    cd 项目名\项目名\spiders

    创建爬虫文件
    scrapy genspider 爬虫文件名 目标网页
    eg：scrapy genspider baidu http://www.baidu.com
    一般目标网页不输入http://，因为start_urls的值是根据allowed_domains修改的

3. 运行爬虫代码
    scrapy crawl 爬虫名
    eg：scrapy crawl baidu
    注意：注释掉settings.py里的ROBOTSTXT_OBEY = True
```

```python
import scrapy


class BaiduSpider(scrapy.Spider):
    # 爬虫的名字，用于运行爬虫的时候，使用的值
    name = 'baidu'
    # 允许访问的域名
    allowed_domains = ['http://www.baidu.com']

    # 起始的url地址，指的是第一次要访问的域名
    # start_urls 是在allowed_domains的前面添加一个http://
    #             在 allowed_domains的后面添加一个/
    start_urls = ['http://www.baidu.com/']

    # 是执行了start_urls之后，执行的方法，方法中的response就是返回的那个对象
    # 相当于 response = urllib.request.urlopen()
    #       response = requests.get()
    def parse(self, response):
        print('我永远喜欢monica')
```



## scrapy_58同城项目结构和基本方法

```
scrapy项目的结构
	项目名字
		项目名字
			spiders文件夹（存储的是爬虫文件）
				init
				自定义的爬虫文件  核心功能文件
			init
			items:定义数据结构的地方爬取的数据都包合哪些
			middleware:中间件  代理
			pipelines:管道  用来处理下载的数据
			settings:配置文件  robots协议  ua定义等
			
response的属性和方法：
response.text获取的是响应的字符串
response.body 获取的是二进制数据
response.xpath 可以接是xpath方法来解析response中的内容
response.extract（）提取seletor对象的data属性值
response.extract_first（）提取的seletor列表的第一个数据
```





## scrapy工作原理

1、引擎向spiders要url
2、引擎将要爬取的url给调度器
3、调度器会将url生成请求对象放入到指定的队列中
4、从队列中出队一个请求
5、引擎将请求交给下载器进行处理
6、下载器发送请求获取互联网数据
7、下载器将数据返回给引擎
8、引擎将数据再次给到spiders
9、spiders通过xpath解析该数据，得到数据或者url
10、spiders将数据或者ur给到引擎
11、引擎判断该数据还是url，是数据，交给管道（item pipeline）处理，是ur交给调度器处理

![image-20220209185233970](C:\Users\ASUS\AppData\Roaming\Typora\typora-user-images\image-20220209185233970.png)





## scrapy shell

```python
进入到scrapy shell的终端直接在windows的终端中输入scrapy shell域名
如果想看到一些高亮或者自动补全那可以安装ipython  (pip install ipython)
```





## scrapy_当当网爬取数据

dang.py

```python
import scrapy
# 导入类，虽然报错但不影响
from scrapy_dangdang_095.items import ScrapyDangdang095Item


class DangSpider(scrapy.Spider):
    name = 'dang'
    allowed_domains = ['http://category.dangdang.com/cp01.01.02.00.00.00.html']
    # 注意删除.html后面的/
    start_urls = ['http://category.dangdang.com/cp01.01.02.00.00.00.html']

    def parse(self, response):
        # pipelines 下载数据
        # items 定义数据结构
        # src = //ul[@id="component_59"]//li//img/@src
        # alt = //ul[@id="component_59"]//li//img/@alt
        # price = //ul[@id="component_59"]//li//p[@class="price"]/span[1]
        # 所有的selector对象都可以再次调用xpath方法
        li_list = response.xpath('//ul[@id="component_59"]//li')

        for li in li_list:

            src = li.xpath('.//img/@data-original').extract_first()
            if src:
                src = src
            else:
                src = li.xpath('.//img/@src').extract_first()

            # 第一张图片没有data-original，跟后面的不一样
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//p[@class="price"]/span[1]/text()').extract_first()

            book = ScrapyDangdang095Item(src=src, name=name, price=price)

            # 获取一个book就将book交给pipelines
            yield book
```

items.py

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class ScrapyDangdang095Item(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    # 通俗的说就是你要下载的数据都有什么

    # 图片
    src = scrapy.Field()
    # 名字
    name = scrapy.Field()
    # 价格
    price = scrapy.Field()
```

pipelines.py

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


# 如果想使用管道的话，就必须在setting中开启管道
class ScrapyDangdang095Pipeline:
    # 在爬虫文件开始之前就执行的一个方法
    def open_spider(self, spider):
        self.fp = open('book.json', 'w', encoding='utf-8')

    # item就是yield后面的book对象
    def process_item(self, item, spider):
        # 以下这种模式不推荐，因为每传递来一个对象就打开一次文件
        # # (1)write方法必须要写一个字符串，不能是其他对象
        # # (2)w模式会每一个对象都打开一次文件，覆盖之前的内容
        # with open('book.json', 'a', encoding='utf-8') as fp:
        #     fp.write(item)

        self.fp.write(str(item))

    # 在爬虫文件执行完之后，执行的方法
    def close_spider(self, spider):
        self.fp.close()
```

settings.py

```python
# 取消这一行的注释
ITEM_PIPELINES = {
    # 管道可以有很多个，那么管道是有优先级的，优先级的范围是1到1000，值越小优先级越高
   'scrapy_dangdang_095.pipelines.ScrapyDangdang095Pipeline': 300,
}
```





## scrapy_当当网开启多条管道下载

```python
# 多条管道开启
# （1）定义管道类
# （2）在settings中开启管道
# 'scrapy_dangdang_095.pipelines.DangDangDownloadPipeline': 301
class DangDangDownloadPipeline:
    def process_item(self, item, spider):
        url = 'http:' + item.get('src')
        filename = './books/' + item.get('name') + '.jpg'

        urllib.request.urlretrieve(url=url, filename=filename)

        return item
```



## scrapy_当当网多页下载

```python
import scrapy
# 导入类，虽然报错但不影响
from scrapy_dangdang_095.items import ScrapyDangdang095Item


class DangSpider(scrapy.Spider):
    name = 'dang'
    # 如果是多页下载的话，那么必须调整allowed_domains的范围，一般情况下只写域名
    allowed_domains = ['category.dangdang.com']
    # 注意删除.html后面的/
    start_urls = ['http://category.dangdang.com/cp01.01.02.00.00.00.html']

    base_url = 'http://category.dangdang.com/pg'
    page = 1

    def parse(self, response):
        # pipelines 下载数据
        # items 定义数据结构
        # src = //ul[@id="component_59"]//li//img/@src
        # alt = //ul[@id="component_59"]//li//img/@alt
        # price = //ul[@id="component_59"]//li//p[@class="price"]/span[1]
        # 所有的selector对象都可以再次调用xpath方法
        li_list = response.xpath('//ul[@id="component_59"]//li')

        for li in li_list:

            src = li.xpath('.//img/@data-original').extract_first()
            if src:
                src = src
            else:
                src = li.xpath('.//img/@src').extract_first()

            # 第一张图片没有data-original，跟后面的不一样
            name = li.xpath('.//img/@alt').extract_first()
            price = li.xpath('.//p[@class="price"]/span[1]/text()').extract_first()

            book = ScrapyDangdang095Item(src=src, name=name, price=price)

            # 获取一个book就将book交给pipelines
            yield book

        # 每一页的爬取的业务逻辑全都是一样的，所以我们只需要执行的那个页的请求再次调用parse方法
        # 第一页：http://category.dangdang.com/cp01.01.02.00.00.00.html
        # 第二页：http://category.dangdang.com/pg2-cp01.01.02.00.00.00.html
        # 第三页：http://category.dangdang.com/pg3-cp01.01.02.00.00.00.html

        if self.page < 100:
            self.page = self.page + 1

            url = self.base_url + str(self.page) + '-cp01.01.02.00.00.00.html'

            # 怎么调用parse方法
            # scrapy.Request就是scrapy的get请求
            # url就是请求地址
            # callback就是你要执行的那个函数，不需要加()
            yield scrapy.Request(url=url, callback=self.parse)

```





## scrapy_电影天堂多页数据下载

mv.py

```python
import scrapy
from scrapy_movie_099.items import ScrapyMovie099Item

class MvSpider(scrapy.Spider):
    name = 'mv'
    allowed_domains = ['www.ygdy8.net']
    start_urls = ['https://www.ygdy8.net/html/gndy/china/index.html']

    def parse(self, response):
        # 要第一页的名字，第二页的图片
        a_list = response.xpath('//ul//table//a[2]')

        for a in a_list:
            # 获取第一页的名字和第二页的地址
            name = a.xpath('./text()').extract_first()
            href = a.xpath('./@href').extract_first()

            # 第二页的地址是
            url = 'https://www.ygdy8.net' + href

            # 对第二页的链接发起访问,注意修改allowed_domains
            yield scrapy.Request(url=url, callback=self.parse_second, meta={'name': name})

    def parse_second(self, response):
        # 注意如果拿不到数据的情况下一定检查你的xpath语法是否正确
        src = response.xpath('//div[@id="Zoom"]//img/@src').extract_first()
        # 接受到请求的那个meta参数的值
        name = response.meta['name']

        moive = ScrapyMovie099Item(src=src, name=name)

        yield moive
```

items.py

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class ScrapyMovie099Item(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    name = scrapy.Field()
    src = scrapy.Field()
```

pipelines.py

```python
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter


class ScrapyMovie099Pipeline:

    def open_spider(self, spider):
        self.fp = open('movie.json', 'w', encoding='utf-8')

    def process_item(self, item, spider):
        self.fp.write(str(item))
        return item

    def close_spider(self, spider):
        self.fp.close()
```





## scrapy_链接提取器的使用

```
3.提取链接
链接提取器，在这里就可以写规则提取指定链接
scrapy.linkextractors.LihkExtractor（
	allow=（），#正则表达式提取符合正则的链接
	deny=（），#（不用）正则表达式不提取符合正则的链接
	allow_domains=（），#（不用）允许的域名			       deny_domains=（），#（不用）不允许的域名
 	restrict_xpaths=（），#xpath，提取符合xpath规则的链接
 	restrict_css=（）#提取符合选择器规则的链接）
4.模拟使用
正则用法：links1=LinkExtractor（allow=r'list_23_\d+\.html'）
xpath用法：links2=LinkExtractor（restrict_xpaths=r"//div[@class="×"]'）
css用法：links3=LinkExtractor（restrict_css='.x'）
5.提取连接
1ink.extract_links（response）
```

